Q1: Link Latency and Throughput
L1 (h1-h2):
- Latency: 170.64 ms avg
- Throughput: 18.95 Mbps
L2 (h2-h3):
- Latency: 123.90 ms avg
- Throughput: 38.19 Mbps
L3 (h3-h4):
- Latency: 206.73 ms avg
- Throughput: 28.47 Mbps
L4 (h2-h5):
- Latency: 101.45 ms avg
- Throughput: 23.91 Mbps
L5 (h3-h6):
- Latency: 102.07 ms avg
- Throughput: 23.90 Mbps

Q2: Path Latency and Throughput (h1 <-> h4)
Prediction:
- Expected RTT: 160 ms ; bcs 80ms one way x 2
- Expected Throughput: 20 Mbps ; limited by L1 bottleneck

Measurements:
- Average RTT (from latency_Q2.txt): 161.522 ms
- Measured Throughput (from throughput_Q2.txt): 22.785 Mbps

Explanation:
Our predicted latency was almost exact to the real average. The throughput however was slightly higher than the 20 Mbps bottleneck we've predicted (by around 3 Mbps). This is because mininet switches buffer packets sometimes, thus allowing short bursts of traffic to exceed the strict limit at the time of the brief tests

Q3: Effects of Multiplexing (s1 <-> s4)
Two simultaneous pairs
Prediction:
- Expected RTT (each pair): 160 ms
- Expected Throughput (each pair): 10 Mbps

Measurements:
- Pair 1 avg RTT: 957.522 ms
- Pair 1 throughput: 6.532 Mbps
- Pair 2 avg RTT: 957.553 ms
- Pair 2 throughput: 5.916 Mbps

Explanation:
The throughput trend matches the multiplexing prediction: with two concurrent flows, each flow gets a larger share than with three flows. The total throughput stays below the ideal bottleneck capacity because TCP, socket pacing, and measurement overhead reduce efficiency in this short run. The RTT is much higher than the base path RTT because both flows queue at shared bottleneck links, increasing queueing delay significantly.

Three simultaneous pairs
Prediction:
- Expected RTT (each pair): 160 ms
- Expected Throughput (each pair): 6.667 Mbps

Measurements:
- Pair 1 avg RTT: 939.034 ms
- Pair 1 throughput: 4.302 Mbps
- Pair 2 avg RTT: 939.297 ms
- Pair 2 throughput: 4.070 Mbps
- Pair 3 avg RTT: 969.030 ms
- Pair 3 throughput: 3.418 Mbps

Explanation:
With three concurrent flows sharing the same bottleneck region, each flow receives a smaller throughput share than in the two-flow case. The measured rates are not exactly equal because TCP does not split perfectly fairly over short experiments and flow startup timing can bias shares. RTT remains very high for all three pairs due to persistent queues at congested links.

Q4: Effects of Latency (h1 <-> h4 and h5 <-> h6)
Prediction:
- h1 <-> h4 expected RTT: 160 ms
- h1 <-> h4 expected Throughput: 20 Mbps
- h5 <-> h6 expected RTT: 40 ms
- h5 <-> h6 expected Throughput: 20 Mbps

Measurements:
- h1 <-> h4 avg RTT (from latency_h1-h4.txt): 974.485 ms
- h1 <-> h4 throughput (from throughput_h1-h4.txt): 3.427 Mbps
- h5 <-> h6 avg RTT (from latency_h5-h6.txt): 393.246 ms
- h5 <-> h6 throughput (from throughput_h5-h6.txt): 22.435 Mbps

Explanation:
These results show a strong latency-dependent fairness effect: the shorter-RTT pair (h5-h6) captures much more throughput than the longer-RTT pair (h1-h4) when both run simultaneously. This is expected for TCP because lower RTT increases ACK feedback rate and effective window growth, so that flow ramps and recovers faster. The RTT values are higher than unloaded-path predictions because concurrent traffic creates queueing delay on shared portions of the topology.
